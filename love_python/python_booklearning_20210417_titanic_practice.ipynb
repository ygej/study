{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 책과 코드 읽기: 파이썬 머신러닝 완벽가이드 \n",
    " - Ch.6 사이킷런으로 수행하는 타이타닉 생존자 예측(p131~ 146)\n",
    " - 타이타닉 탑승자 데이터를 기반으로 생존자 예측하기\n",
    " - 목표: 예측 정확도 82%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "pd.set_option('display.max_rows', 500) # row 한번에 여러개 보기\n",
    "pd.set_option('display.max_columns', 100) # 컬럼 한번에 여러개 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale = 1.5)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA SET (step.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"C:/Users/User/Downloads/data/titanic/train.csv\")\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"C:/Users/User/Downloads/data/titanic/test.csv\")\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA INFO FIND NULL DATA (step.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA INFO\n",
    "train.info()\n",
    "# 'Survived' 컬럼을 통해 탑승자의 생사여부를 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIND NULL DATA\n",
    "train.isnull().sum()\n",
    "# 'Age', 'Cabin', 'Embarked' 컬럼에 결손값이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum()\n",
    "# 'Age', 'Fare', 'Cabin' 컬럼에 결손값이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore (step.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. SEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data = train, x = 'Sex', hue = 'Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(data= train, \n",
    "               index = 'Sex', \n",
    "               values = 'Survived', \n",
    "               aggfunc = [np.mean, np.sum])\n",
    "# train 데이터 셋에서 여성의 74% (233명), 남성의 18%(109명)이 생존했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data = train, x = 'Pclass', hue = 'Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(data = train, \n",
    "               index = 'Pclass', \n",
    "               values = 'Survived', \n",
    "               aggfunc = [np.mean, np.sum])\n",
    "# train 데이터 셋에서 Pclass 1의 62% (136명), 2의 47%(87명), 3의 24%(119명)이 생존했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data = train, x = 'Embarked', hue = 'Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(data = train, \n",
    "               index = 'Embarked', \n",
    "               values = \"Survived\", \n",
    "               aggfunc =[np.sum, np.mean])\n",
    "# train 데이터 셋에서 Embarked C의 55% (93명), Q의 38%(30명), S의 33%(217명)이 생존했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Age&Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data = train, \n",
    "           x = \"Age\", \n",
    "           y = \"Fare\", \n",
    "           hue = 'Survived', \n",
    "           fit_reg = False)\n",
    "# Fare 가 500 이상인 특잇값(outliers)이 존재합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = sns.FacetGrid(train, hue = 'Survived', aspect = 3)\n",
    "fa.map(sns.kdeplot, 'Age')\n",
    "fa.add_legend()\n",
    "# 정규분포 모양을 보이지만, 어린이들의 생존율이 높습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = sns.FacetGrid(train, hue = 'Survived', aspect = 5)\n",
    "fa.map(sns.kdeplot, 'Fare')\n",
    "fa.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. SibSp & Parch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Family_size'] = train['SibSp'] + train[\"Parch\"] + 1\n",
    "print(train.shape)\n",
    "train[['SibSp', 'Parch', 'Family_size']].head()\n",
    "# 함께 탄 가족의 숫자 컬럼을 새로 만듭니다.\n",
    "# 직계 가족만 포함되기에, 친적들과 같이 탑승한 경우는 확인이 어렵습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data = train, x = 'Family_size', hue = \"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(data = train, \n",
    "               index = \"Family_size\", \n",
    "               values = 'Survived', \n",
    "               aggfunc = [np.mean, np.sum])\n",
    "# 1인, 5인 이상 가족의 생존율이 낮습니다. 2인 이상 4인 이하 가족의 생존율이 높습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['Title'] = train['Name'].str.split(',')\n",
    "train['Title'] = train['Name'].str.split(',').str[1].str.split('.').str[0].str.strip()\n",
    "print(train.shape)\n",
    "train[[\"Name\", \"Title\"]].head()\n",
    "# 이름에 들어간 Mr, Mrs 등으로 결혼 여부, 사회적 지위 등을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data = train, x = 'Title', hue = 'Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(data = train, \n",
    "               index = 'Title', \n",
    "               values = 'Survived', \n",
    "               aggfunc = ['mean', 'sum', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 가설 검증 1차 (4.12)\n",
    "\n",
    "- 1. Age에 결손값이 있으면 생존율이 낮을 것이다.\n",
    "- 2. 선실 등급 & 성별에 따라 생존율 차이가 있을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가설 검증 1차 (4.12) : 1. Age에 결손값이 있으면 생존율이 낮을 것이다 -> True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Age_null'] = pd.isna(train['Age'])\n",
    "print(train.shape)\n",
    "train[['Age_null', 'Age']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data = train, x = 'Age_null', hue = \"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(data = train, \n",
    "               index = [\"Age_null\"], \n",
    "               values = 'Survived', \n",
    "               aggfunc = [np.mean, np.sum])\n",
    "# Age에 결손값이 있는 경우 생존율이 29% 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(data = train, \n",
    "               index = [\"Sex\", \"Pclass\", \"Age_null\"], \n",
    "               values = 'Survived', \n",
    "               aggfunc = [np.mean, np.sum])\n",
    "# 여성의 경우 나이에 결손값과 여부가 생존에 큰 영향을 미치지 않지만, 3등석 남성의 경우.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가설 검증 1차 (4.12) : 2. 선실 등급 & 성별에 따라 생존율 차이가 있을 것이다. -> True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(data = train, \n",
    "               index = ['Sex', 'Pclass'], \n",
    "               values = 'Survived', \n",
    "               aggfunc = [np.sum, np.mean])\n",
    "# 3등석의 여성의 경우 생존률이 50% 입니다. 1등석의 남성의 경우 생존율이 36% 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사망한 3등석 여성의 경우 어떤 특징을?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclass3_female = train[(train['Sex'] == 'female') & (train['Pclass'] == 3)]\n",
    "pd.pivot_table(data = pclass3_female, \n",
    "               index = [\"Embarked\", 'Family_size'], \n",
    "               values = 'Survived', \n",
    "               aggfunc = [np.mean, np.sum])\n",
    "# Embarked 'S'에서 탑승한 3등석 여성 승객의 경우 생존률이 낮다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 가설 검증 2차 (4.14)\n",
    "\n",
    "- 1. Pcalss에서 Fare에 따라 생존률이 다를 것이다.\n",
    "- 2. Age에 따라 생존률이 다를 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(data = train, \n",
    "               index = ['Sex', 'Pclass'], \n",
    "               values = 'Survived', \n",
    "               aggfunc = [np.sum, np.mean, 'count'])\n",
    "# Sex에 따른 Pclass 별 평균 생존률  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Fare'].value_counts()\n",
    "# 티겟 가격이 248개로 나눠져 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, (ax1) = plt.subplots(nrows=1, ncols=1)\n",
    "figure.set_size_inches(32, 8)\n",
    "sns.countplot(data = train, x = 'Fare', hue = 'Survived', ax = ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 Fare에 대해 유독 생존률이 낮다 왜 그럴까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_pivot = pd.pivot_table(data = train, \n",
    "                            index = ['Pclass','Sex','Fare'], \n",
    "                            values = 'Survived', \n",
    "                            aggfunc = [np.sum, np.mean, 'count'])\n",
    "\n",
    "fare_pivot.sort_values(by = ('mean', 'Survived'), ascending=False).sort_index()\n",
    "# 같은 Pclass, Sex에도 특정 Fare에 따라 결정적으로 생존률이 나뉩니다. \n",
    "# 아마도 Fare에 따라 배정된 방이 다르고 방에 위치에 따라 생존률이 결정된 것으로 보입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age에 따른 생존률\n",
    "age_pivot = pd.pivot_table(data = train[train['Age_null'] == False], \n",
    "               index = ['Age'], \n",
    "               values = 'Survived', \n",
    "               aggfunc = [np.sum, np.mean, 'count'])\n",
    "\n",
    "age_pivot.sort_values(by = ('mean', 'Survived'), ascending=False).sort_index()\n",
    "# 대체적으로 6세 이하 승객들의 생존률이 높습니다.\n",
    "# 2세 승객들은 어떤 이유에서 생존률이 낮을까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_pivot = pd.pivot_table(data = train[train['Age_null'] == False], \n",
    "               index = ['Family_size','Age'], \n",
    "               values = 'Survived', \n",
    "               aggfunc = [np.sum, np.mean, 'count'])\n",
    "\n",
    "age_pivot.sort_values(by = ('mean', 'Survived'), ascending=False).sort_index()\n",
    "# 답은 가족 숫자였습니다. 4인 가족 이하 9세 이하 승객들의 생존률이 높습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결론 및 전처리 계획:\n",
    "1. null data: train 'Age' , test 'Fare'\n",
    "2. 'Family_size' 1 = 'small'  / 2~4 = 'middel' / 5 ~ 'big' one hot encoding\n",
    "3. 'Name' - Master T/F\n",
    "4. 'small_family_baby' = 'Age' + 'Family_size' under 9 & 'small' T/F\n",
    "\n",
    "5. lable encoding\n",
    "6. one hot encoding\n",
    "\n",
    "7. 'Fare' = good_ticket / bad_ticket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. null data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 'Age'\n",
    "train[\"Age_fillin\"] = train[\"Age\"]\n",
    "train.loc[train['Age'].isnull(), 'Age_fillin'] = train['Age'].mean()\n",
    "train.loc[train['Age'].isnull(), ['Age', 'Age_fillin']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 'Age'\n",
    "test[\"Age_fillin\"] = test[\"Age\"]\n",
    "test.loc[test['Age'].isnull(), 'Age_fillin'] = test['Age'].mean()\n",
    "test.loc[test['Age'].isnull(), ['Age', 'Age_fillin']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 'Fare'\n",
    "train[\"Fare_fillin\"] = train[\"Fare\"]\n",
    "train.loc[train['Fare'].isnull(), 'Fare_fillin'] = train['Fare'].mean()\n",
    "train.loc[train['Fare'].isnull(), ['Fare', 'Fare_fillin']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 'Fare'\n",
    "test[\"Fare_fillin\"] = test[\"Fare\"]\n",
    "test.loc[test['Fare'].isnull(), 'Fare_fillin'] = test['Fare'].mean()\n",
    "test.loc[test['Fare'].isnull(), ['Fare', 'Fare_fillin']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Family size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Family_size\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\n",
    "print(train.shape)\n",
    "train[[\"SibSp\", \"Parch\", \"Family_size\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"Family_size\"] = test[\"SibSp\"] + test[\"Parch\"] + 1\n",
    "print(test.shape)\n",
    "test[[\"SibSp\", \"Parch\", \"Family_size\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['Family_size'] == 1, 'Family_size_name'] = 'single'\n",
    "train.loc[(train['Family_size'] > 1) & (train['Family_size'] < 5), 'Family_size_name'] = 'small'\n",
    "train.loc[train['Family_size'] > 4, 'Family_size_name'] = 'big'\n",
    "\n",
    "train[['Family_size', 'Family_size_name']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[test['Family_size'] == 1, 'Family_size_name'] = 'single'\n",
    "test.loc[(test['Family_size'] > 1) & (test['Family_size'] < 5), 'Family_size_name'] = 'small'\n",
    "test.loc[test['Family_size'] > 4, 'Family_size_name'] = 'big'\n",
    "\n",
    "test[['Family_size', 'Family_size_name']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_got_train_Family_size_name = pd.get_dummies(train['Family_size_name'], prefix = 'Family_size_name')\n",
    "one_got_train_Family_size_name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_got_test_Family_size_name = pd.get_dummies(test['Family_size_name'], prefix = 'Family_size_name')\n",
    "one_got_test_Family_size_name.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Title'] = train['Name'].str.split(',').str[1].str.split('.').str[0].str.strip()\n",
    "\n",
    "print(train.shape)\n",
    "train[['Name', 'Title']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Title'] = test['Name'].str.split(',').str[1].str.split('.').str[0].str.strip()\n",
    "\n",
    "print(test.shape)\n",
    "test[['Name', 'Title']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Master\"] = train[\"Title\"].str.contains(\"Master\")\n",
    "print(train.shape)\n",
    "train[['Master', 'Name']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"Master\"] = test[\"Title\"].str.contains(\"Master\")\n",
    "print(test.shape)\n",
    "test[['Master', 'Name']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. small_family_baby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['small_family_baby'] = (train['Family_size'] == 'small') & (train['Age_fillin'] > 10)\n",
    "train[['small_family_baby', 'Family_size','Age_fillin']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['small_family_baby'] = (test['Family_size'] == 'small') & (test['Age_fillin'] > 10)\n",
    "test[['small_family_baby', 'Family_size','Age_fillin']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_got_train_Pclass = pd.get_dummies(train['Pclass'], prefix = 'Pclass')\n",
    "print(one_got_train_Pclass.shape)\n",
    "one_got_train_Pclass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_got_test_Pclass = pd.get_dummies(test['Pclass'], prefix = 'Pclass')\n",
    "print(one_got_test_Pclass.shape)\n",
    "one_got_test_Pclass.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_concat = pd.concat([train,one_got_train_Pclass, one_got_train_Family_size_name], axis = 1)\n",
    "print(train_concat.shape)\n",
    "train_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_concat = pd.concat([test,one_got_test_Pclass, one_got_test_Family_size_name], axis = 1)\n",
    "print(test_concat.shape)\n",
    "test_concat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_concat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"Sex\",  \n",
    "                 'small_family_baby', \n",
    "                 'Family_size_name_big',\n",
    "                 'Family_size_name_single', \n",
    "                 'Family_size_name_small',\n",
    "                 'Pclass_1',\n",
    "                 'Pclass_2', \n",
    "                 'Pclass_3',\n",
    "                 \"Master\", ]\n",
    "\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_concat.copy()\n",
    "df['target'] = df['Survived']\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name = \"target\"\n",
    "label_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_names = train_concat[feature_names]\n",
    "test_feature_names = test_concat[feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes_train = train_feature_names.dtypes\n",
    "encoders = {}\n",
    "for column in train_feature_names.columns:\n",
    "    if str(dtypes_train[column]) == 'object':\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(train_feature_names[column])\n",
    "        encoders[column] = encoder\n",
    "        \n",
    "df_train = train_feature_names.copy()        \n",
    "for column in encoders.keys():\n",
    "    encoder = encoders[column]\n",
    "    df_train[column] = encoder.transform(train_feature_names[column])\n",
    "\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes_test = test_feature_names.dtypes\n",
    "encoders = {}\n",
    "for column in test_feature_names.columns:\n",
    "    if str(dtypes_test[column]) == 'object':\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(test_feature_names[column])\n",
    "        encoders[column] = encoder\n",
    "        \n",
    "df_test = test_feature_names.copy()        \n",
    "for column in encoders.keys():\n",
    "    encoder = encoders[column]\n",
    "    df_test[column] = encoder.transform(test_feature_names[column])\n",
    "\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.copy()\n",
    "print(X_train.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test.copy()\n",
    "print(X_test.shape)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df[label_name]\n",
    "print(y_train.shape)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(max_depth=8, random_state=0)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "print(predictions.shape)\n",
    "predictions[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"C:/Users/User/Downloads/data/titanic/gender_submission.csv\", index_col=\"PassengerId\")\n",
    "print(submission.shape)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"Survived\"] = predictions\n",
    "\n",
    "print(submission.shape)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"C:/Users/User/Downloads/data/titanic/20210418_gender_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 import\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# 기타 라이브러리\n",
    "import random\n",
    "import gc\n",
    "import os\n",
    "\n",
    "\n",
    "# 0. 쓰고싶은 모델을 Import 한다.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 1. 모델들과 결과들의 Dictionary를 만들어준다.\n",
    "models_list = {'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "              'RandomForestClassifier': RandomForestClassifier(),\n",
    "              'svm':svm.SVC(),\n",
    "              'SGDClassifier':SGDClassifier(),\n",
    "              'LogisticRegression':LogisticRegression()}\n",
    "\n",
    "\n",
    "# 2. 클래스로 만들어 보기\n",
    "\n",
    "class AutoML:\n",
    "    \n",
    "    def __init__(self, data, target,test_size, model):\n",
    "        \n",
    "        # 모델 리스트\n",
    "        models_list = {'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "              'RandomForestClassifier': RandomForestClassifier(),\n",
    "              'svm':svm.SVC(),\n",
    "              'SGDClassifier':SGDClassifier(),\n",
    "              'LogisticRegression':LogisticRegression()}\n",
    "        \n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.test_size = test_size\n",
    "        self.model = models_list[model]\n",
    "        self.results = dict()\n",
    "        \n",
    "        # Feature, target 나누기\n",
    "        X = self.data\n",
    "        Y = self.target\n",
    "        \n",
    "        # train, test 데이터 나누기\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X,\n",
    "                                                           Y,\n",
    "                                                           test_size = self.test_size,\n",
    "                                                           random_state = 31)\n",
    "    \n",
    "    def fit(self):\n",
    "        self.fit = self.model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "    def predict(self):\n",
    "        self.predict = self.model.predict(self.X_test)\n",
    "        \n",
    "    def show(self):\n",
    "        print('accuracy_score:',accuracy_score(self.y_test, self.predict))\n",
    "        \n",
    "    def kfold(self, nfold):\n",
    "        self.nfold = nfold\n",
    "        folds = KFold(n_splits = nfold)\n",
    "        splits = folds.split(self.X_train, self.y_train)\n",
    "        columns = self.X_train.columns\n",
    "        y_preds = np.zeros(self.X_test.shape[0])\n",
    "        y_oof = np.zeros(self.X_train.shape[0])\n",
    "        score = 0\n",
    "        \n",
    "        \n",
    "        for fold_n, (trn_idx, val_idx)in enumerate(splits):\n",
    "            X_trn, X_val = self.X_train[columns].iloc[trn_idx], self.X_train[columns].iloc[val_idx]\n",
    "            y_trn, y_val = self.y_train.iloc[trn_idx], self.y_train.iloc[val_idx]\n",
    "            \n",
    "            self.model.fit(self.X_train, self.y_train)\n",
    "            \n",
    "            y_pred_val = self.model.predict(X_val)\n",
    "            y_pred_val = [int(v >= 0.5) for v in y_pred_val]\n",
    "            y_oof[val_idx] = y_pred_val\n",
    "            \n",
    "            print(f\"Fold {fold_n + 1} | F1 Score: {f1_score(y_val, y_pred_val, average='weighted')}\")\n",
    "    \n",
    "            score += f1_score(y_val, y_pred_val, average='weighted') / self.nfold\n",
    "            y_preds += self.model.predict(self.X_test) / self.nfold\n",
    "    \n",
    "            del X_trn, X_val, y_trn, y_val\n",
    "            gc.collect()\n",
    "            \n",
    "        print(f\"\\nMean F1 score = {score}\")\n",
    "        \n",
    "        \n",
    "    def Coarse_Finer_Search(self):\n",
    "        n_estimators = 300\n",
    "        num_epoch = 100\n",
    "        coarse_hyperparameters_list = []\n",
    "\n",
    "        for epoch in range(num_epoch):\n",
    "            max_depth = np.random.randint(low=2, high=100)\n",
    "            max_features = np.random.uniform(low=0.1, high=1.0)\n",
    "\n",
    "            model = RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                  max_depth=max_depth,\n",
    "                                  max_features=max_features,\n",
    "                                  n_jobs=-1,\n",
    "                                  random_state=37)\n",
    "            \n",
    "            score = cross_val_score(model, \n",
    "                                    self.data, self.target, \n",
    "                                    cv=20).mean()\n",
    "    \n",
    "            # hyperparameter 탐색 결과를 딕셔너리화 합니다.\n",
    "            hyperparameters = {'epoch': epoch,\n",
    "                               'score': score,\n",
    "                               'n_estimators': n_estimators,\n",
    "                               'max_depth': max_depth,\n",
    "                               'max_features': max_features\n",
    "                              }\n",
    "\n",
    "            # hyperparameter 탐색 결과를 리스트에 저장합니다.\n",
    "            coarse_hyperparameters_list.append(hyperparameters)\n",
    "\n",
    "            # hyperparameter 탐색 결과를 출력합니다.\n",
    "            print(f\"{epoch:2} n_estimators = {n_estimators}, max_depth = {max_depth:2}, max_features = {max_features:.6f}, Score = {score:.5f}\")\n",
    "\n",
    "        # coarse_hyperparameters_list를 Pandas의 DataFrame으로 변환합니다.\n",
    "        coarse_hyperparameters_list = pd.DataFrame.from_dict(coarse_hyperparameters_list)\n",
    "\n",
    "        # 변환한 coarse_hyperparameters_list를 score가 높은 순으로 정렬합니다.\n",
    "        coarse_hyperparameters_list = coarse_hyperparameters_list.sort_values(by=\"score\", ascending = True)\n",
    "\n",
    "        # coarse_hyperparameters_list 변수에 할당된 데이터의 행렬 사이즈를 출력합니다.\n",
    "        print(coarse_hyperparameters_list.shape)\n",
    "\n",
    "        # coarse_hyperparameters_list의 상위 10개를 출력합니다.\n",
    "        coarse_hyperparameters_list.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = AutoML(df_train, df.target, 0.3, 'RandomForestClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.fit()\n",
    "test.predict()\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.kfold(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.Coarse_Finer_Search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
